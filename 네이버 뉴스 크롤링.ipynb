{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad28f2e8",
   "metadata": {},
   "source": [
    "# 필요 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f679858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04aa571",
   "metadata": {},
   "source": [
    "# 검색어 바탕 네이버 기사 url 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49fa00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 291/291 [02:04<00:00,  2.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 252/252 [01:51<00:00,  2.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 398/398 [02:34<00:00,  2.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 254/254 [01:31<00:00,  2.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 304/304 [01:54<00:00,  2.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 265/265 [01:40<00:00,  2.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 349/349 [02:01<00:00,  2.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 281/281 [01:41<00:00,  2.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 264/264 [01:29<00:00,  2.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 351/351 [02:01<00:00,  2.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 238/238 [01:23<00:00,  2.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 269/269 [01:37<00:00,  2.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 247/247 [01:28<00:00,  2.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 214/214 [01:02<00:00,  3.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 275/275 [01:18<00:00,  3.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 240/240 [01:09<00:00,  3.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 311/311 [01:32<00:00,  3.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 317/317 [01:32<00:00,  3.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 581/581 [02:54<00:00,  3.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 302/302 [01:31<00:00,  3.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 268/268 [01:18<00:00,  3.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 329/329 [01:37<00:00,  3.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 393/393 [01:59<00:00,  3.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 281/281 [01:23<00:00,  3.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 301/301 [01:27<00:00,  3.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 286/286 [01:18<00:00,  3.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 275/275 [01:20<00:00,  3.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [01:14<00:00,  3.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 294/294 [01:24<00:00,  3.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 288/288 [01:26<00:00,  3.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 364/364 [01:47<00:00,  3.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 310/310 [01:30<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 219/219 [01:55<00:00,  1.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 254/254 [01:14<00:00,  3.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 280/280 [01:21<00:00,  3.43it/s]\n",
      " 77%|█████████████████████████████████████████████████████████████▉                  | 179/231 [00:53<00:15,  3.36it/s]"
     ]
    }
   ],
   "source": [
    "def crawler(maxpage, query, s_date, e_date):\n",
    "    naver_urls = []\n",
    "    s_from = s_date.replace(\".\", \"\")\n",
    "    e_to = e_date.replace(\".\", \"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1    \n",
    "    \n",
    "    while page < maxpage_t:\n",
    "        \n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + query + \"&ds=\" + s_date + \"&de=\" + e_date +  \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        # ua = UserAgent()\n",
    "        # headers = {'User-Agent' : ua.random}\n",
    "\n",
    "        req = requests.get(url)\n",
    "        \n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "        \n",
    "        for urls in soup.select(\"a.info\"):\n",
    "\n",
    "\n",
    "            try:\n",
    "                if \"news.naver.com\" in urls['href']:\n",
    "                    naver_urls.append(urls['href'])\n",
    "                  \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        page += 10\n",
    "        \n",
    "    return naver_urls\n",
    "    \n",
    "max_page = 400\n",
    "query = '오피스 부동산'\n",
    "s_date = '2019.01.01'\n",
    "e_date = '2019.01.31'\n",
    "year = ['2019','2020','2021']\n",
    "month = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "for j in list(range(0,3)):\n",
    "    s_date = s_date.split(\".\")\n",
    "    s_date[0] = year[j]\n",
    "    s_date = \".\".join(s_date)\n",
    "    \n",
    "    e_date = e_date.split(\".\")\n",
    "    e_date[0] = year[j]\n",
    "    e_date = \".\".join(e_date)\n",
    "    for i in list(range(0,12)):\n",
    "        s_date = s_date.split(\".\")\n",
    "        s_date[1] = month[i]\n",
    "        s_date =  \".\".join(s_date)\n",
    "\n",
    "        \n",
    "        e_date = e_date.split(\".\")\n",
    "        e_date[1] = month[i]\n",
    "        e_date =  \".\".join(e_date)\n",
    "        \n",
    "        total_urls = crawler(max_page,query,s_date,e_date)\n",
    "        \n",
    "        \n",
    "        total_urls = list(set(total_urls))\n",
    "        \n",
    "        # ConnectionError방지\n",
    "\n",
    "        headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\" }\n",
    "\n",
    "\n",
    "        titles = []\n",
    "        contents=[]\n",
    "        for i in tqdm(total_urls):\n",
    "            original_html = requests.get(i,headers=headers)\n",
    "            html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "            # 검색결과확인시\n",
    "            #print(html)\n",
    "\n",
    "            #뉴스 제목 가져오기\n",
    "            title = html.select(\"div#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "            # list합치기\n",
    "            title = ''.join(str(title))\n",
    "            # html태그제거\n",
    "            pattern1 = '<[^>]*>'\n",
    "            title = re.sub(pattern=pattern1,repl='',string=title)\n",
    "            titles.append(title)\n",
    "\n",
    "            #뉴스 본문 가져오기\n",
    "\n",
    "            content = html.select(\"div#dic_area\")\n",
    "\n",
    "            # 기사 텍스트만 가져오기\n",
    "            # list합치기\n",
    "            content = ''.join(str(content))\n",
    "\n",
    "            #html태그제거 및 텍스트 다듬기\n",
    "            content = re.sub(pattern=pattern1,repl='',string=content)\n",
    "            pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "            content = content.replace(pattern2,'')\n",
    "\n",
    "            contents.append(content)\n",
    "\n",
    "        # 데이터프레임으로 정리(titles,url,contents)\n",
    "\n",
    "\n",
    "        news_df = pd.DataFrame({'title': titles, 'link': total_urls, 'content': contents})\n",
    "        news_df.to_csv(f\"news_{query}from_{s_date}to_{e_date}.csv\" ,index=False, encoding='utf-8-sig')\n",
    "# # 검색어 및 날짜지정    \n",
    "# maxpage = input(\"검색 할 페이지수: \")\n",
    "# query = input(\"검색어: \")\n",
    "# s_date = input(\"시작 날짜(YYYY.MM.DD): \")\n",
    "# e_date = input(\"종료 날짜(YYYY.MM.DD): \")\n",
    "# crawler(maxpage, query, s_date, e_date)\n",
    "# print(len(naver_urls))\n",
    "# print(len(list(set(naver_urls))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1c2d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.01\n",
      "2019.01.31\n",
      "2019.02.01\n",
      "2019.02.31\n",
      "2019.03.01\n",
      "2019.03.31\n",
      "2019.04.01\n",
      "2019.04.31\n",
      "2019.05.01\n",
      "2019.05.31\n",
      "2019.06.01\n",
      "2019.06.31\n",
      "2019.07.01\n",
      "2019.07.31\n",
      "2019.08.01\n",
      "2019.08.31\n",
      "2019.09.01\n",
      "2019.09.31\n",
      "2019.10.01\n",
      "2019.10.31\n",
      "2019.11.01\n",
      "2019.11.31\n",
      "2019.12.01\n",
      "2019.12.31\n",
      "2020.01.01\n",
      "2020.01.31\n",
      "2020.02.01\n",
      "2020.02.31\n",
      "2020.03.01\n",
      "2020.03.31\n",
      "2020.04.01\n",
      "2020.04.31\n",
      "2020.05.01\n",
      "2020.05.31\n",
      "2020.06.01\n",
      "2020.06.31\n",
      "2020.07.01\n",
      "2020.07.31\n",
      "2020.08.01\n",
      "2020.08.31\n",
      "2020.09.01\n",
      "2020.09.31\n",
      "2020.10.01\n",
      "2020.10.31\n",
      "2020.11.01\n",
      "2020.11.31\n",
      "2020.12.01\n",
      "2020.12.31\n"
     ]
    }
   ],
   "source": [
    "s_date = '2019.01.01'\n",
    "e_date = '2019.01.31'\n",
    "year = ['2019','2020']\n",
    "month = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "for j in list(range(0,2)):\n",
    "    s_date = s_date.split(\".\")\n",
    "    s_date[0] = year[j]\n",
    "    s_date = \".\".join(s_date)\n",
    "    \n",
    "    e_date = e_date.split(\".\")\n",
    "    e_date[0] = year[j]\n",
    "    e_date = \".\".join(e_date)\n",
    "    for i in list(range(0,12)):\n",
    "\n",
    "\n",
    "        s_date = s_date.split(\".\")\n",
    "        s_date[1] = month[i]\n",
    "        s_date =  \".\".join(s_date)\n",
    "        print(s_date)\n",
    "\n",
    "        \n",
    "        e_date = e_date.split(\".\")\n",
    "        e_date[1] = month[i]\n",
    "        e_date =  \".\".join(e_date)\n",
    "        print(e_date)\n",
    "#         crawler(max_page,query,s_date,e_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc048c",
   "metadata": {},
   "source": [
    "# 해당 url의 제목, 본문 내용 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098922de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ConnectionError방지\n",
    "\n",
    "headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\" }\n",
    "\n",
    "\n",
    "titles = []\n",
    "contents=[]\n",
    "for i in tqdm(naver_urls):\n",
    "    original_html = requests.get(i,headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "    # 검색결과확인시\n",
    "    #print(html)\n",
    "    \n",
    "    #뉴스 제목 가져오기\n",
    "    title = html.select(\"div#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    # list합치기\n",
    "    title = ''.join(str(title))\n",
    "    # html태그제거\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1,repl='',string=title)\n",
    "    titles.append(title)\n",
    "\n",
    "    #뉴스 본문 가져오기\n",
    "\n",
    "    content = html.select(\"div#dic_area\")\n",
    "\n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    content = ''.join(str(content))\n",
    "    \n",
    "    #html태그제거 및 텍스트 다듬기\n",
    "    content = re.sub(pattern=pattern1,repl='',string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2,'')\n",
    "\n",
    "    contents.append(content)\n",
    "\n",
    "# 데이터프레임으로 정리(titles,url,contents)\n",
    "\n",
    "\n",
    "news_df = pd.DataFrame({'title': titles, 'link': naver_urls, 'content': contents})\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449aadd",
   "metadata": {},
   "source": [
    "# 뉴스 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643cb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s_date)\n",
    "news_df.to_csv(f\"news_{query}from_{s_date}to_{e_date}.csv\" ,index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb33670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 할 페이지수: 400\n",
      "검색어: 리테일 부동산\n",
      "시작 날짜(YYYY.MM.DD): 2019.01.01\n",
      "종료 날짜(YYYY.MM.DD): 2019.01.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 163/163 [00:46<00:00,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "def crawler(maxpage, query, s_date, e_date):\n",
    "    naver_urls = []\n",
    "    s_from = s_date.replace(\".\", \"\")\n",
    "    e_to = e_date.replace(\".\", \"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1    \n",
    "    \n",
    "    while page < maxpage_t:\n",
    "        \n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + query + \"&ds=\" + s_date + \"&de=\" + e_date +  \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        # ua = UserAgent()\n",
    "        # headers = {'User-Agent' : ua.random}\n",
    "\n",
    "        req = requests.get(url)\n",
    "        \n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "        \n",
    "        for urls in soup.select(\"a.info\"):\n",
    "\n",
    "\n",
    "            try:\n",
    "                if \"news.naver.com\" in urls['href']:\n",
    "                    naver_urls.append(urls['href'])\n",
    "                  \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        page += 10\n",
    "        \n",
    "    return naver_urls\n",
    "# 검색어 및 날짜지정    \n",
    "maxpage = input(\"검색 할 페이지수: \")\n",
    "query = input(\"검색어: \")\n",
    "s_date = input(\"시작 날짜(YYYY.MM.DD): \")\n",
    "e_date = input(\"종료 날짜(YYYY.MM.DD): \")\n",
    "total_urls = crawler(maxpage,query,s_date,e_date)\n",
    "\n",
    "\n",
    "total_urls = list(set(total_urls))\n",
    "headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\" }\n",
    "\n",
    "\n",
    "titles = []\n",
    "contents=[]\n",
    "for i in tqdm(total_urls):\n",
    "    original_html = requests.get(i,headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "    # 검색결과확인시\n",
    "    #print(html)\n",
    "\n",
    "    #뉴스 제목 가져오기\n",
    "    title = html.select(\"div#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    # list합치기\n",
    "    title = ''.join(str(title))\n",
    "    # html태그제거\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1,repl='',string=title)\n",
    "    titles.append(title)\n",
    "\n",
    "    #뉴스 본문 가져오기\n",
    "\n",
    "    content = html.select(\"div#dic_area\")\n",
    "\n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    content = ''.join(str(content))\n",
    "\n",
    "    #html태그제거 및 텍스트 다듬기\n",
    "    content = re.sub(pattern=pattern1,repl='',string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2,'')\n",
    "\n",
    "    contents.append(content)\n",
    "\n",
    "# 데이터프레임으로 정리(titles,url,contents)\n",
    "\n",
    "\n",
    "news_df = pd.DataFrame({'title': titles, 'link': total_urls, 'content': contents})\n",
    "news_df.to_csv(f\"news_{query}from_{s_date}to_{e_date}.csv\" ,index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a503b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
